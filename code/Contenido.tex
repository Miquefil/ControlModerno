% \tableofcontents
% \newpage

%Definiciónes de comandos y snippets

%SNIPPETS IMPORTANTES:
% gr: pone figuras al 75% de escala
% eq: pone ecuación de linea $$
% mat: matriz de corchetes cuadrados 3x3
% fd: math inline
% ds: display math
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Comienzo de contenido

\section{Matrices}
\begin{definition}
    El rango de una matriz $A$ de $m \cdot n$ es igual al $min(m, n)$.
\end{definition}
\textbf{Propiedades de la multiplicación:}
\begin{bangenumerate}
    \item $(AB)C = A(BC)$
    \item $A(B+C)= AB + AC$
    \item! $AB \neq BA$, en general
\end{bangenumerate}
\subsection{Operación determinante}
\begin{definition}
    \textbf{Método de los Cofactores:} Sea $A$ una matriz de dimensiones $n \times n$, 
    podemos entonces calcular el determinante de $A$ ($\determinant(A)$ o $|A|$) como sigue.
    Definimos el cofactor de fila $i$ y columna $j$ como:
    \begin{equation*}
        A_{ij} = (-1)^{i+j} \; |M_{ij}|,
    \end{equation*}
    donde $|M_{ij}|$ es el determinante de la matriz $M_{ij}$, que es la matriz de orden $(n-1)\cdot(n-1)$ resultante
    de quitar la i-ésima fila y j-ésima columna de $A$. Luego el determinante de A es la suma de los productos de los
    elemntos de una fila o columna con sus cofactores, es decir.
    \begin{equation*}
        |A| = a_{k1} \cdot A_{k1} + \dots + a_{kn} \cdot A_{kn},
    \end{equation*}
    o también,
    \begin{equation*}
        |A| = a_{1k} \cdot A_{1k} + \dots + a_{nk}\cdot A_{nk}, \; \forall k \in {1, \dots, n}.
    \end{equation*}
\end{definition}

\begin{example}[Filas o columnas de ceros]
    El cálculo de
    \begin{equation*}
        |A| = 
        \begin{vmatrix}
            \sigma & \omega & 0 \\
            \omega & \sigma & 0 \\
            0 & 0 & \lambda \\
        \end{vmatrix},
    \end{equation*}
    se puede realizar rápidamente por método de cofactores, realizando el método en la tercer fila o columna, quedando:
    \begin{equation*}
        |A| = (-1)^{3+3}\lambda \cdot \begin{vmatrix}
            \sigma & \omega \\
            \omega & \sigma \\
        \end{vmatrix} = \lambda \; (\sigma^2 - \omega^2).
    \end{equation*}
\end{example}

\textbf{Propiedades:} Sean A y B matrices cuadradas, entonces

\begin{bangenumerate}
    \item $|A| = |A^t|$,
    \item $|A \cdot B| = |A|\cdot |B|$,
    \item $|A^{-1}| = |A|^{-1}$,
    \item Si $A$ es triangular o diagonal entonces $|A|$ es igual al producto de
    su diagonal principal.
\end{bangenumerate}

\subsection{Operación inversa}
\begin{definition}
    Si $A$ es una matriz no nula de dimensión $n \times n$ entonces
    \begin{equation*}
        A^{-1}  = \frac{Adj(A)^{T}}{|A|},
    \end{equation*}
    donde $Adj(A)$ es la matriz adjunta, es decir aquella que está compuesta por los cofactores de A.
\end{definition}

\textbf{Propiedades:}
\begin{bangenumerate}
    \item $(A^{-1})^{-1} = A$
    \item! $(A\cdot B)^{-1} = B^{-1}\cdot A^{-1}$
    \item $(A^T)^{-1} = (A^{-1})^T$
\end{bangenumerate}

\begin{teorema}
    Una matriz tiene inversa si y sólo si su determinante es distinto
    de cero
    \begin{equation*}
        \exists A^{-1} \Leftrightarrow |A| \neq 0
    \end{equation*}
\end{teorema}


\subsection{Operación Trasposición}
La trasposición de una matriz ($n \times m$), rectangular o cuadrada, es la
reflexión de los elementos respecto de su diagonal principal, adquiriendo
la forma ($m \cdot n$).
\begin{equation*}
    \begin{bmatrix}
        a & b & c \\
        d & e & f \\
    \end{bmatrix}^{T}  =
    \begin{bmatrix}
        a & b \\
        c & d \\
        e & f \\
    \end{bmatrix}
\end{equation*}

\textbf{Propiedades:}
\begin{bangenumerate}
    \item Involutiva: $(A^t)^t = A$
    \item Distributiva: $(A+B)^t = A^t + B^t$
    \item! Producto: $(A \cdot B)^t = B^t \cdot A^t$
\end{bangenumerate}


\newpage
\section{Espacios Vectoriales y Transformaciones Lineales}
\begin{definition}
    Sea $ \bm{X} = \{ \bm{x_1, x_2} ,\dots, \bm{x_n} \}$ un conjunto de 
    vectores que pertenece a un cierto espacio vectorial.
    \begin{equation*}
        \bm{v} = \alpha_1 \cdot \bm{x_1} + \alpha_2 \cdot \bm{x_2} + \dots + \alpha_n \times \bm{x_n}    
    \end{equation*}
    Se dice que $\bm{v}$ es \textit{Combinación Lineal} del conjunto $\bm{X}$.
\end{definition}
\begin{definition}
    Si la única solución a la combinación del vector nulo,
    \begin{equation*}
        \bm{0} = \alpha_1 \cdot \bm{x_1} + \alpha_2 \cdot \bm{x_2} + \dots + \alpha_n \times \bm{x_n} \;,
    \end{equation*}
    es $\alpha_i = 0$, entonces se dice que el conjunto $\bm{X}$ está compuesto
    por vectores \textit{Linealmente Independientes}.\\
    En otras palabras, se dice que un conjunto de vectores son linealmente independientes si ningún vector del conjunto
    puede ser escrito como combinación lineal del resto.
\end{definition}
\begin{obs}
    Por consecuencia
    \begin{equation*}
       \exists \; \alpha_i / \;\; \bm{x_k} = \alpha_1 \cdot \bm{x_1} + \alpha_2 \cdot \bm{x_2} + \dots + \alpha_n \times \bm{x_n}\; , \;\; \bm{x_k} \in X   
    \end{equation*}
    entonces $X$ es un conjunto de vectores \textit{Linealmente Dependientes.}\\
    De esto también se desprende que $\bm{0}$ no puede ser parte de un conjunto
    de vectores linealmente independientes.
\end{obs} 

\begin{definition}
    Sea $S$ un subespacio de $V$, y sea $B = \{ \bm{b_1 b_2} $ \dots $\bm{b_m} \}$.
    Luego B es una \textbf{base de S} si:
    \begin{enumerate}
        \item $\bm{b_1 b_2}$ \dots $\bm{b_m}$ son linealmente independientes.
        \item $\bm{b_1 b_2}$ \dots $\bm{b_m}$ generan al subespacio $S$.
    \end{enumerate}
\end{definition}

\subsection{Transformación Lineal}
Queremos enconrtar una matriz $A$ que al multiplicarla por las coordenadas de un
vector $\bm{v}$ en la base $B_1$ ([$\bm{v}]_{B1}$) de como resultado el transformado de
dicho vector escrito en base $B_2$. Es decir:
\begin{equation*}
    [T(\bm{v})]_{B2} = \bm{A} \cdot [\bm{v}]_{B1}
\end{equation*}
donde $T: V \rightarrow W$ tal que $B_1$ es base de $V$ y $B_2$ es base de $W$

\begin{teorema}
    Sean $V$ y $W$ espacios vectoriales de dimensión finita. Supongamos $B_1$
    base de $V$ y $B_2$ base de $W$. Existe una \textbf{única} matriz $A \in M_{n\times n}(\mathbb{R})$ tal que:
    \begin{equation}
        \label{eqn: matrizTransformación}
        [T(\bm{v})]_{B2} = \bm{A} \cdot [\bm{v}]_{B1}
    \end{equation}
    donde $A$ se suele notar como [$T]_{B1\rightarrow B2}$
\end{teorema}
Para obtener esta matriz sólamente necesitamos calcular los tranformados
de la base $B_1 = \{ \bm{v_1} \bm{v_2}$ \dots $\bm{v_n}\}$
\begin{equation}
    \bm{A} = [T]_{B1\rightarrow B2} = 
    \begin{bmatrix}
    \uparrow & \uparrow &  & \uparrow\\
    [T(\bm{v_1})]_{B2} & [T(\bm{v_2})]_{B2} & \dots & [T(\bm{v_n})]_{B2}\\
    \downarrow & \downarrow &  & \downarrow\\
    \end{bmatrix}
\end{equation}
Esta matriz toma vectores en base $B_1$ los transforma y devuelve sus coordenadas
en base $B_2$.\\

\begin{teorema}
    Sean $V$ y $W$ espacios vectoriales de dimensión finita. Sean
    $B_1$ y $B_{1'}$ dos bases de V, y $B_2$ y $B_{2'}$ dos bases 
    de W, donde $T: V \rightarrow W$, entonces
    \begin{equation}
        \label{eqn:camino}
        [T]_{B_{1'} \rightarrow B_{2'}} = [B_2]_{B_{2'}} \;
        [T]_{B_{1} \rightarrow B_{2}} \; [B_{1'}]_{B_1} 
    \end{equation}
\end{teorema}


\vspace*{2pt}

Observemos que por lo tanto, si llamo $\bm{\tilde{A}} = [T]_{B_{1'} \rightarrow B_{2'}}$
entonces podemos escribir (\ref*{eqn:camino}) como
\begin{equation}
    \bm{\tilde{A}} = [B_2]_{B_{2'}} \cdot \bm{A} \cdot [B_{1'}]_{B_1} 
\end{equation}

\vspace*{3pt}

\begin{example}
    Sea $T: \mathbb{R}^3 \rightarrow \mathbb{R}^3 / \; \; T(x, y, z) = (x+y, z)$.
    Si $B = \{(1,0,2);(0,2,1);(0,0,3)\}$ es una base de $\mathbb{R}^3$, hallar $[T]_{B \rightarrow C}$ (siendo C la base canonica)
    y $[T( \bm{v})]_C$ siendo $[\bm{v}]_B = [1,2,0]$

    \vspace*{5pt}

    Sabemos que
    \begin{equation*}
        [T]_{B \rightarrow C} = 
        \begin{bmatrix}
            \uparrow & \uparrow &  \uparrow\\
            [T(1,0,2)]_{C} & [T(0,2,1)]_{C} & [T(0,0,3)]_{C}\\
            \downarrow & \downarrow &  \downarrow\\
        \end{bmatrix}
        =
        \begin{bmatrix}
        1 & 2 & 0\\
        2 & 1 & 3\\
        \end{bmatrix}
    \end{equation*} 

    Luego utilizando la matriz hallada podemos obtener el vector transformado en base
    canonica
    \begin{equation*}
        \begin{split}
            [T(\bm{v})]_C = [T]_{B \rightarrow C} \cdot [\bm{v}]_B \\
            [T(\bm{v})]_C = (5, 4) 
        \end{split} 
    \end{equation*}
    que puede ser verificado buscando $T(\bm{v})$.
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Autovalores y Autovectores}
\begin{definition}
    Para una matriz $ \bm{A} $, un vector no nulo $ \bm{x}$ se dice que 
    es un autovector de $ \bm{A}$ si
    \begin{equation*}
        \bm{A} \cdot \bm{x} = \lambda \bm{x} 
    \end{equation*}
\end{definition}
\begin{obs}
    Si partimos de la definición entonces podemos operar de la siguiente forma
    \begin{equation*}
        \begin{split}
            \bm{0} &= \lambda \bm{x} - \bm{A} \bm{x}\\
            \bm{0} &= (\lambda \bm{I} - \bm{A})\cdot \bm{x}\\
        \end{split}
    \end{equation*}
    y como $\bm{x}$ es no nulo, el sistema tiene una solución no trivial, por lo que
    \begin{equation}
        \label{eqn:autovalores}
        | \lambda \bm{I} - \bm{A} | = 0
    \end{equation}
    donde el determinante se llama \textit{polinomio carácteristico}, y es un polinomio
    mónico de orden $n$. 
\end{obs}

\begin{definition}
    Si $A \in M_n(\mathbb{R})$ entonces:
\end{definition}
\begin{bangenumerate}
    \item! $\lambda$ es autovalor de $A \Leftrightarrow P(\lambda)=  | \lambda \bm{I} - \bm{A} | = 0$
    es decir, los autovalores son las raices del polinomio característico.
    \item! Si $\lambda$ es autovalor de $A \Rightarrow$ las soluciones no nulas
    del sistema homogeneo $(\bm{A}-\lambda \bm{I})\bm{x}=\bm{0}$ son los
    autovectores asociados a $\lambda$
\end{bangenumerate}

\begin{obs}
    Podemos ver de la definición que los autovectores no son únicos, ya que
    si $\bm{x}$ es un autovector, entonces $\alpha\bm{x}$ también lo es.
\end{obs}

\begin{teorema}
    Si $\lambda$ es un autovalor de $A$ entonces $\alpha + \lambda$ es 
    un autovalor de $\alpha I + A$
\end{teorema}

\begin{definition}
    Sea $A$ una matriz de $n \times n$, y $T$ una matriz no singular también
    de $n \times n$, y sea $\tilde{A} = T \; A \; T^{-1} $, entonces $A$ y $\tilde{A}$
    se dicen \textit{matrices semejantes}. Están relacionadas por la matriz de 
    semejanza $T$.
\end{definition}

\begin{teorema}
    Dos matrices semejantes tienen los mismos autovalores
\end{teorema}

\begin{obs}
    Como toda \textit{TL} $T:V\rightarrow V$ tiene asociada una matriz ($[T]_B$),
    las definiciones de autovalores y autovectores también las podemos aplicar a una matriz.
\end{obs}
\vspace*{1pt}

\begin{example}[]
    Sea $B$ una matriz triángular de orden $3\times 3$ genérica, buscar sus
    autovalores y autovectores:
    \vspace*{5pt}
    Si llamamos
    \begin{equation*}
        B = 
        \begin{bmatrix}
        a & b & c\\
        0 & e & f\\
        0 & 0 & i\\
        \end{bmatrix}
    \end{equation*}
    entonces el polinomio característico queda definido por 
    $P(\lambda) = (\lambda - a)\cdot (\lambda - e) \cdot (\lambda - i)$ por lo tanto
    los autovalores de la matriz B son $\{a, e, i\}$.

    Luego podemos obtener los autovectores planteando lo siguiente
    \begin{equation*}
        \begin{split}
            (\bm{B} - \lambda \bm{I}) \bm{x} &= \bm{0} \\
            \begin{bmatrix}
            a-\lambda & b & c\\
            0 & e-\lambda & f\\
            0 & 0 & i-\lambda\\
            \end{bmatrix} \cdot
            \begin{bmatrix}
            x_1\\
            x_2\\
            x_3\\
            \end{bmatrix} &= \bm{0}
        \end{split}
    \end{equation*}
    por lo que obtenemos el siguiente sistema
    \begin{equation}
        \begin{cases}
            (a-\lambda)\cdot x_1 + b\cdot x_2 + c \cdot x_3 &= 0\\
            (e-\lambda)\cdot x_2 + f \cdot x_3 &= 0\\
            (i-\lambda)\cdot x_3 &=0 \\
        \end{cases}
    \end{equation}
    que define al espacio de autovectores asociado al autovalor lambda.

\end{example}
